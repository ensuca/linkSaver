# ğŸ”— LinkSaver

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![BeautifulSoup](https://img.shields.io/badge/BeautifulSoup-4-orange.svg)](https://www.crummy.com/software/BeautifulSoup/)
[![Requests](https://img.shields.io/badge/Requests-latest-red.svg)](https://requests.readthedocs.io/)

> A lightweight, efficient web scraping utility that extracts and exports all hyperlinks from any website into a structured text file.

---

## ğŸ“‹ Overview

**LinkSaver** is a Python-based web scraping tool designed to streamline the process of extracting hyperlinks from websites. Whether you're conducting SEO audits, performing competitive analysis, building sitemaps, or validating external links, LinkSaver provides a quick and reliable solution for link extraction.

This tool demonstrates practical application of web scraping techniques using industry-standard libraries, showcasing skills in HTTP request handling, HTML parsing, and data extractionâ€”essential competencies for modern software development and data engineering roles.

### ğŸ¯ Use Cases

- **SEO Analysis**: Extract all internal and external links for comprehensive site audits
- **Link Validation**: Identify broken links and outdated references across web properties
- **Competitive Research**: Analyze competitor website structures and link strategies
- **Sitemap Generation**: Create preliminary data for sitemap construction
- **Content Migration**: Document existing link structures before website redesigns
- **Academic Research**: Collect reference links from research databases or documentation sites

---

## âœ¨ Key Features

- **ğŸš€ Simple & Fast**: Extract all links from a webpage with a single command
- **ğŸ¯ Comprehensive Extraction**: Captures all `<a>` tag href attributes from target URLs
- **ğŸ“ Clean Output**: Exports links to organized, line-separated text files for easy processing
- **ğŸ”§ Minimal Setup**: Only two lightweight dependencies required
- **ğŸ’» Cross-Platform**: Works seamlessly on Windows, macOS, and Linux
- **ğŸ” HTML Parser**: Leverages BeautifulSoup4's robust parsing capabilities for reliable extraction
- **ğŸŒ HTTP Handling**: Built on the Requests library for stable and efficient web requests
- **ğŸ“¦ Lightweight**: Minimal resource footprint, perfect for automation and batch processing

---

## ğŸ› ï¸ Tech Stack

### Core Technologies
- **Python 3.7+**: Modern Python leveraging clean syntax and robust standard libraries
- **Requests**: Industry-standard HTTP library for making web requests with elegant API
- **BeautifulSoup4**: Powerful HTML/XML parsing library for web scraping and data extraction

### Design Approach
This project follows a functional programming paradigm with a focus on simplicity and maintainability. The architecture prioritizes:
- **Separation of Concerns**: Distinct function for link extraction logic
- **Reusability**: Modular design allows easy integration into larger projects
- **Readability**: Clean, self-documenting code following Python best practices (PEP 8)

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.7 or higher installed on your system
- pip (Python package manager)
- Internet connection for web requests

### Installation

1. **Clone the Repository**
```bash
git clone https://github.com/ensuca/linkSaver.git
cd linkSaver
```

2. **Install Dependencies**
```bash
pip install requests
pip install beautifulsoup4
```

Or install both at once:
```bash
pip install requests beautifulsoup4
```

### Configuration

Before running the script, configure your target URL:

1. Open `linkSaver.py` in your preferred text editor
2. Locate line 18:
```python
url = 'https://www.example.com'  # Write the address of the website which you want the link addresses
```
3. Replace `'https://www.example.com'` with your target website URL
4. (Optional) Modify the output filename on line 19 if desired:
```python
output_file = 'links.txt'
```

---

## ğŸ’¡ Usage

### Basic Usage

After configuring your target URL, simply run:

```bash
python linkSaver.py
```

The script will:
1. Send an HTTP GET request to the specified URL
2. Parse the HTML response
3. Extract all `href` attributes from `<a>` tags
4. Save the results to `links.txt` in the current directory

### Example

```python
# Configure your target in linkSaver.py
url = 'https://www.python.org'
output_file = 'python_links.txt'

# Run the script
save_links_to_file(url, output_file)
```

**Output** (`python_links.txt`):
```
https://www.python.org/about/
https://www.python.org/downloads/
https://www.python.org/doc/
https://www.python.org/community/
/success-stories/
...
```

### Integration Example

LinkSaver can be easily integrated into larger projects:

```python
from linkSaver import save_links_to_file

# Extract links from multiple sites
websites = [
    'https://example1.com',
    'https://example2.com',
    'https://example3.com'
]

for i, site in enumerate(websites):
    save_links_to_file(site, f'links_site_{i}.txt')
    print(f"âœ… Extracted links from {site}")
```

---

## ğŸ“ Project Structure

```
linkSaver/
â”œâ”€â”€ linkSaver.py        # Main script containing link extraction logic
â”œâ”€â”€ README.md           # Project documentation (this file)
â”œâ”€â”€ README.txt          # Legacy documentation
â”œâ”€â”€ .gitattributes      # Git configuration
â””â”€â”€ links.txt           # Output file (generated after running)
```

### Core Components

**`save_links_to_file(url, output_file)`**
- **Parameters**:
  - `url` (str): Target website URL to scrape
  - `output_file` (str): Path for output text file
- **Returns**: None
- **Functionality**: Orchestrates HTTP request, HTML parsing, link extraction, and file writing

**Location**: linkSaver.py:5

---

## ğŸ” How It Works

### Technical Implementation

1. **HTTP Request**: Uses `requests.get()` to fetch the HTML content from the target URL
2. **HTML Parsing**: BeautifulSoup parses the raw HTML into a navigable tree structure
3. **Link Extraction**: The `.find_all('a')` method locates all anchor tags in the document
4. **Attribute Extraction**: Extracts `href` attributes using `.get('href')` with null checking
5. **File Output**: Writes each link to a new line in the specified output file

### Code Flow

```
User Input (URL)
    â†“
HTTP GET Request (requests)
    â†“
HTML Response
    â†“
Parse HTML (BeautifulSoup)
    â†“
Extract <a> tags
    â†“
Get href attributes
    â†“
Write to File (links.txt)
    â†“
Complete âœ…
```

---

## âš¡ Performance Considerations

- **Lightweight**: Minimal memory footprint, suitable for scraping large pages
- **Efficient Parsing**: BeautifulSoup4's lxml parser (if available) provides optimal performance
- **Single Request**: One HTTP request per execution minimizes network overhead
- **Stream Writing**: File is written incrementally, preventing memory issues with large link sets

### Typical Execution Time
- Small website (<100 links): < 1 second
- Medium website (100-1000 links): 1-3 seconds
- Large website (1000+ links): 3-10 seconds

*Performance varies based on network speed and target server response time*

---

## ğŸ§ª Testing

### Manual Testing Procedure

1. **Test with a known website**:
```bash
# Set url = 'https://www.python.org' in linkSaver.py
python linkSaver.py
```

2. **Verify output**:
```bash
cat links.txt  # Linux/Mac
type links.txt  # Windows
```

3. **Validate link count**:
```python
with open('links.txt', 'r') as f:
    link_count = len(f.readlines())
    print(f"Extracted {link_count} links")
```

### Test Cases

âœ… **Valid Website URL**: Successfully extracts links
âœ… **HTTPS URLs**: Handles secure connections
âœ… **Relative Links**: Captures partial URLs (e.g., `/about`, `#contact`)
âœ… **Empty href**: Filters out null/empty href attributes
âœ… **Large Pages**: Processes pages with 1000+ links

---

## ğŸš€ Deployment & Automation

### Scheduled Execution (Linux/Mac)

Add to crontab for daily link extraction:
```bash
0 2 * * * cd /path/to/linkSaver && /usr/bin/python3 linkSaver.py
```

### Windows Task Scheduler

Create a batch file (`run_linksaver.bat`):
```batch
@echo off
cd C:\Projects\linkSaver
python linkSaver.py
```

Schedule via Task Scheduler for automated execution.

### Docker Deployment

```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY linkSaver.py .
RUN pip install requests beautifulsoup4
CMD ["python", "linkSaver.py"]
```

---

## ğŸ—ºï¸ Roadmap & Future Enhancements

### Planned Features

- [ ] **Command-Line Interface**: Add argparse for URL and output file arguments
- [ ] **Error Handling**: Implement try-except blocks for network errors and invalid URLs
- [ ] **Link Validation**: Add HTTP status code checking for each extracted link
- [ ] **Format Options**: Support JSON, CSV, and XML output formats
- [ ] **Recursive Crawling**: Option to follow links and extract from multiple pages
- [ ] **Filter Options**: Filter by domain, protocol (http/https), or URL patterns
- [ ] **Progress Indicator**: Add progress bar for large sites using tqdm
- [ ] **Async Requests**: Implement async HTTP requests for faster bulk scraping
- [ ] **GUI Interface**: Tkinter-based desktop application for non-technical users
- [ ] **Link Categorization**: Classify links as internal, external, images, or documents
- [ ] **robots.txt Compliance**: Respect website crawling rules automatically
- [ ] **Rate Limiting**: Implement delays to avoid overwhelming target servers

### Known Limitations

- No error handling for network failures or invalid URLs
- Cannot process JavaScript-rendered content (SPA applications)
- No support for authentication-required pages
- Extracts all links without filtering duplicates
- Relative URLs are not converted to absolute URLs

*These limitations represent opportunities for enhancement and demonstrate awareness of production-grade requirements*

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!

### How to Contribute

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Development Guidelines

- Follow PEP 8 style guidelines
- Add comments for complex logic
- Test changes before submitting PRs
- Update README for new features

---

## ğŸ“„ License

This project is licensed under the MIT License - feel free to use, modify, and distribute as needed.

---

## ğŸ‘¨â€ğŸ’» Author

**Enes Uca**

- ğŸŒ Portfolio: [ensuca.github.io](https://ensuca.github.io/ensuca.githubio)
- ğŸ’¼ LinkedIn: [linkedin.com/in/enes-uca-41039327b](https://www.linkedin.com/in/enes-uca-41039327b)
- ğŸ™ GitHub: [@ensuca](https://github.com/ensuca)

---

## ğŸ™ Acknowledgments

- **BeautifulSoup4**: Leonard Richardson for creating this excellent parsing library
- **Requests**: Kenneth Reitz for the elegant HTTP library
- **Python Community**: For comprehensive documentation and support

---

## ğŸ“ Support

If you encounter issues or have questions:

1. Check existing [GitHub Issues](https://github.com/ensuca/linkSaver/issues)
2. Create a new issue with detailed description
3. Contact via LinkedIn for professional inquiries

---

<div align="center">

**â­ Star this repository if you found it helpful!**

*Built with Python and passion for clean code*

</div>

---
---

# ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/lisans-MIT-green.svg)](LICENSE)
[![BeautifulSoup](https://img.shields.io/badge/BeautifulSoup-4-orange.svg)](https://www.crummy.com/software/BeautifulSoup/)
[![Requests](https://img.shields.io/badge/Requests-latest-red.svg)](https://requests.readthedocs.io/)

> Herhangi bir web sitesinden tÃ¼m baÄŸlantÄ±larÄ± Ã§Ä±karÄ±p yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir metin dosyasÄ±na aktaran hafif ve verimli bir web kazÄ±ma aracÄ±.

---

## ğŸ“‹ Genel BakÄ±ÅŸ

**LinkSaver**, web sitelerinden baÄŸlantÄ± Ã§Ä±karma sÃ¼recini kolaylaÅŸtÄ±rmak iÃ§in tasarlanmÄ±ÅŸ Python tabanlÄ± bir web kazÄ±ma aracÄ±dÄ±r. Ä°ster SEO denetimi yapÄ±yor olun, ister rekabet analizi, site haritasÄ± oluÅŸturma veya harici baÄŸlantÄ± doÄŸrulama yapÄ±yor olun, LinkSaver baÄŸlantÄ± Ã§Ä±karma iÃ§in hÄ±zlÄ± ve gÃ¼venilir bir Ã§Ã¶zÃ¼m sunar.

Bu araÃ§, endÃ¼stri standardÄ± kÃ¼tÃ¼phaneler kullanarak web kazÄ±ma tekniklerinin pratik uygulamasÄ±nÄ± gÃ¶sterir ve HTTP istek yÃ¶netimi, HTML ayrÄ±ÅŸtÄ±rma ve veri Ã§Ä±karma becerilerini sergilerâ€”bu beceriler modern yazÄ±lÄ±m geliÅŸtirme ve veri mÃ¼hendisliÄŸi rolleri iÃ§in temel yetkinliklerdir.

### ğŸ¯ KullanÄ±m AlanlarÄ±

- **SEO Analizi**: KapsamlÄ± site denetimleri iÃ§in tÃ¼m dahili ve harici baÄŸlantÄ±larÄ± Ã§Ä±karma
- **BaÄŸlantÄ± DoÄŸrulama**: Web varlÄ±klarÄ±nda kÄ±rÄ±k baÄŸlantÄ±larÄ± ve gÃ¼ncel olmayan referanslarÄ± belirleme
- **Rekabet AraÅŸtÄ±rmasÄ±**: Rakip web sitesi yapÄ±larÄ±nÄ± ve baÄŸlantÄ± stratejilerini analiz etme
- **Site HaritasÄ± OluÅŸturma**: Site haritasÄ± oluÅŸturmak iÃ§in Ã¶n veri hazÄ±rlama
- **Ä°Ã§erik TaÅŸÄ±ma**: Web sitesi yeniden tasarÄ±mlarÄ± Ã¶ncesi mevcut baÄŸlantÄ± yapÄ±larÄ±nÄ± belgeleme
- **Akademik AraÅŸtÄ±rma**: AraÅŸtÄ±rma veritabanlarÄ±ndan veya dokÃ¼mantasyon sitelerinden referans baÄŸlantÄ±larÄ± toplama

---

## âœ¨ Temel Ã–zellikler

- **ğŸš€ Basit ve HÄ±zlÄ±**: Tek bir komutla web sayfasÄ±ndaki tÃ¼m baÄŸlantÄ±larÄ± Ã§Ä±karma
- **ğŸ¯ KapsamlÄ± Ã‡Ä±karma**: Hedef URL'lerden tÃ¼m `<a>` etiketi href Ã¶zelliklerini yakalama
- **ğŸ“ Temiz Ã‡Ä±ktÄ±**: BaÄŸlantÄ±larÄ± kolay iÅŸleme iÃ§in dÃ¼zenli, satÄ±r bazlÄ± metin dosyalarÄ±na aktarma
- **ğŸ”§ Minimal Kurulum**: Sadece iki hafif baÄŸÄ±mlÄ±lÄ±k gerekiyor
- **ğŸ’» Ã‡apraz Platform**: Windows, macOS ve Linux'ta sorunsuz Ã§alÄ±ÅŸma
- **ğŸ” HTML AyrÄ±ÅŸtÄ±rÄ±cÄ±**: GÃ¼venilir Ã§Ä±karma iÃ§in BeautifulSoup4'Ã¼n saÄŸlam ayrÄ±ÅŸtÄ±rma yeteneklerinden yararlanma
- **ğŸŒ HTTP YÃ¶netimi**: KararlÄ± ve verimli web istekleri iÃ§in Requests kÃ¼tÃ¼phanesi Ã¼zerine inÅŸa edilmiÅŸ
- **ğŸ“¦ Hafif**: Minimal kaynak kullanÄ±mÄ±, otomasyon ve toplu iÅŸleme iÃ§in mÃ¼kemmel

---

## ğŸ› ï¸ Teknoloji YÄ±ÄŸÄ±nÄ±

### Ana Teknolojiler
- **Python 3.7+**: Temiz sÃ¶zdizimi ve saÄŸlam standart kÃ¼tÃ¼phanelerden yararlanan modern Python
- **Requests**: Zarif API ile web istekleri yapmak iÃ§in endÃ¼stri standardÄ± HTTP kÃ¼tÃ¼phanesi
- **BeautifulSoup4**: Web kazÄ±ma ve veri Ã§Ä±karma iÃ§in gÃ¼Ã§lÃ¼ HTML/XML ayrÄ±ÅŸtÄ±rma kÃ¼tÃ¼phanesi

### TasarÄ±m YaklaÅŸÄ±mÄ±
Bu proje, basitlik ve sÃ¼rdÃ¼rÃ¼lebilirliÄŸe odaklanan fonksiyonel programlama paradigmasÄ±nÄ± takip eder. Mimari ÅŸu Ã¶ncelikleri benimser:
- **EndiÅŸelerin AyrÄ±lmasÄ±**: BaÄŸlantÄ± Ã§Ä±karma mantÄ±ÄŸÄ± iÃ§in ayrÄ± fonksiyon
- **Yeniden KullanÄ±labilirlik**: ModÃ¼ler tasarÄ±m, daha bÃ¼yÃ¼k projelere kolay entegrasyona olanak tanÄ±r
- **Okunabilirlik**: Python en iyi uygulamalarÄ±nÄ± (PEP 8) takip eden temiz, kendi kendini belgeleyen kod

---

## ğŸš€ BaÅŸlarken

### Ã–n Gereksinimler

- Sisteminizde Python 3.7 veya Ã¼zeri kurulu olmalÄ±
- pip (Python paket yÃ¶neticisi)
- Web istekleri iÃ§in internet baÄŸlantÄ±sÄ±

### Kurulum

1. **Depoyu KlonlayÄ±n**
```bash
git clone https://github.com/ensuca/linkSaver.git
cd linkSaver
```

2. **BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin**
```bash
pip install requests
pip install beautifulsoup4
```

Ya da ikisini birden yÃ¼kleyin:
```bash
pip install requests beautifulsoup4
```

### YapÄ±landÄ±rma

BetiÄŸi Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce hedef URL'nizi yapÄ±landÄ±rÄ±n:

1. `linkSaver.py` dosyasÄ±nÄ± tercih ettiÄŸiniz metin dÃ¼zenleyicide aÃ§Ä±n
2. 18. satÄ±rÄ± bulun:
```python
url = 'https://www.example.com'  # BaÄŸlantÄ± adreslerini istediÄŸiniz web sitesinin adresini yazÄ±n
```
3. `'https://www.example.com'` ifadesini hedef web sitesi URL'nizle deÄŸiÅŸtirin
4. (Ä°steÄŸe baÄŸlÄ±) Ä°sterseniz 19. satÄ±rda Ã§Ä±ktÄ± dosya adÄ±nÄ± deÄŸiÅŸtirin:
```python
output_file = 'links.txt'
```

---

## ğŸ’¡ KullanÄ±m

### Temel KullanÄ±m

Hedef URL'nizi yapÄ±landÄ±rdÄ±ktan sonra, basitÃ§e Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python linkSaver.py
```

Betik ÅŸunlarÄ± yapacak:
1. Belirtilen URL'ye HTTP GET isteÄŸi gÃ¶nderecek
2. HTML yanÄ±tÄ±nÄ± ayrÄ±ÅŸtÄ±racak
3. `<a>` etiketlerinden tÃ¼m `href` Ã¶zelliklerini Ã§Ä±karacak
4. SonuÃ§larÄ± mevcut dizinde `links.txt` dosyasÄ±na kaydedecek

### Ã–rnek

```python
# linkSaver.py dosyasÄ±nda hedefinizi yapÄ±landÄ±rÄ±n
url = 'https://www.python.org'
output_file = 'python_links.txt'

# BetiÄŸi Ã§alÄ±ÅŸtÄ±rÄ±n
save_links_to_file(url, output_file)
```

**Ã‡Ä±ktÄ±** (`python_links.txt`):
```
https://www.python.org/about/
https://www.python.org/downloads/
https://www.python.org/doc/
https://www.python.org/community/
/success-stories/
...
```

### Entegrasyon Ã–rneÄŸi

LinkSaver, daha bÃ¼yÃ¼k projelere kolayca entegre edilebilir:

```python
from linkSaver import save_links_to_file

# Birden fazla siteden baÄŸlantÄ± Ã§Ä±karma
websites = [
    'https://example1.com',
    'https://example2.com',
    'https://example3.com'
]

for i, site in enumerate(websites):
    save_links_to_file(site, f'links_site_{i}.txt')
    print(f"âœ… {site} adresinden baÄŸlantÄ±lar Ã§Ä±karÄ±ldÄ±")
```

---

## ğŸ“ Proje YapÄ±sÄ±

```
linkSaver/
â”œâ”€â”€ linkSaver.py        # BaÄŸlantÄ± Ã§Ä±karma mantÄ±ÄŸÄ±nÄ± iÃ§eren ana betik
â”œâ”€â”€ README.md           # Proje dokÃ¼mantasyonu (bu dosya)
â”œâ”€â”€ README.txt          # Eski dokÃ¼mantasyon
â”œâ”€â”€ .gitattributes      # Git yapÄ±landÄ±rmasÄ±
â””â”€â”€ links.txt           # Ã‡Ä±ktÄ± dosyasÄ± (Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra oluÅŸturulur)
```

### Ana BileÅŸenler

**`save_links_to_file(url, output_file)`**
- **Parametreler**:
  - `url` (str): KazÄ±nacak hedef web sitesi URL'si
  - `output_file` (str): Ã‡Ä±ktÄ± metin dosyasÄ± yolu
- **DÃ¶nÃ¼ÅŸ**: None
- **Ä°ÅŸlevsellik**: HTTP isteÄŸi, HTML ayrÄ±ÅŸtÄ±rma, baÄŸlantÄ± Ã§Ä±karma ve dosya yazmayÄ± dÃ¼zenler

**Konum**: linkSaver.py:5

---

## ğŸ” NasÄ±l Ã‡alÄ±ÅŸÄ±r

### Teknik Uygulama

1. **HTTP Ä°steÄŸi**: Hedef URL'den HTML iÃ§eriÄŸini almak iÃ§in `requests.get()` kullanÄ±r
2. **HTML AyrÄ±ÅŸtÄ±rma**: BeautifulSoup, ham HTML'yi gezinilebilir bir aÄŸaÃ§ yapÄ±sÄ±na ayrÄ±ÅŸtÄ±rÄ±r
3. **BaÄŸlantÄ± Ã‡Ä±karma**: `.find_all('a')` metodu belgede tÃ¼m anchor etiketlerini bulur
4. **Ã–zellik Ã‡Ä±karma**: Null kontrolÃ¼ ile `.get('href')` kullanarak `href` Ã¶zelliklerini Ã§Ä±karÄ±r
5. **Dosya Ã‡Ä±ktÄ±sÄ±**: Her baÄŸlantÄ±yÄ± belirtilen Ã§Ä±ktÄ± dosyasÄ±nda yeni bir satÄ±ra yazar

### Kod AkÄ±ÅŸÄ±

```
KullanÄ±cÄ± Girdisi (URL)
    â†“
HTTP GET Ä°steÄŸi (requests)
    â†“
HTML YanÄ±tÄ±
    â†“
HTML AyrÄ±ÅŸtÄ±rma (BeautifulSoup)
    â†“
<a> Etiketlerini Ã‡Ä±kar
    â†“
href Ã–zelliklerini Al
    â†“
Dosyaya Yaz (links.txt)
    â†“
TamamlandÄ± âœ…
```

---

## âš¡ Performans DeÄŸerlendirmeleri

- **Hafif**: Minimal bellek kullanÄ±mÄ±, bÃ¼yÃ¼k sayfalarÄ± kazÄ±mak iÃ§in uygun
- **Verimli AyrÄ±ÅŸtÄ±rma**: BeautifulSoup4'Ã¼n lxml ayrÄ±ÅŸtÄ±rÄ±cÄ±sÄ± (mevcutsa) optimal performans saÄŸlar
- **Tek Ä°stek**: YÃ¼rÃ¼tme baÅŸÄ±na bir HTTP isteÄŸi, aÄŸ yÃ¼kÃ¼nÃ¼ minimize eder
- **AkÄ±ÅŸ Yazma**: Dosya artÄ±mlÄ± olarak yazÄ±lÄ±r, bÃ¼yÃ¼k baÄŸlantÄ± setleriyle bellek sorunlarÄ±nÄ± Ã¶nler

### Tipik Ã‡alÄ±ÅŸtÄ±rma SÃ¼resi
- KÃ¼Ã§Ã¼k web sitesi (<100 baÄŸlantÄ±): < 1 saniye
- Orta web sitesi (100-1000 baÄŸlantÄ±): 1-3 saniye
- BÃ¼yÃ¼k web sitesi (1000+ baÄŸlantÄ±): 3-10 saniye

*Performans, aÄŸ hÄ±zÄ±na ve hedef sunucu yanÄ±t sÃ¼resine gÃ¶re deÄŸiÅŸir*

---

## ğŸ§ª Test

### Manuel Test ProsedÃ¼rÃ¼

1. **Bilinen bir web sitesi ile test edin**:
```bash
# linkSaver.py iÃ§inde url = 'https://www.python.org' ayarlayÄ±n
python linkSaver.py
```

2. **Ã‡Ä±ktÄ±yÄ± doÄŸrulayÄ±n**:
```bash
cat links.txt  # Linux/Mac
type links.txt  # Windows
```

3. **BaÄŸlantÄ± sayÄ±sÄ±nÄ± doÄŸrulayÄ±n**:
```python
with open('links.txt', 'r') as f:
    link_count = len(f.readlines())
    print(f"{link_count} baÄŸlantÄ± Ã§Ä±karÄ±ldÄ±")
```

### Test SenaryolarÄ±

âœ… **GeÃ§erli Web Sitesi URL'si**: BaÄŸlantÄ±larÄ± baÅŸarÄ±yla Ã§Ä±karÄ±r
âœ… **HTTPS URL'leri**: GÃ¼venli baÄŸlantÄ±larÄ± iÅŸler
âœ… **GÃ¶receli BaÄŸlantÄ±lar**: KÄ±smi URL'leri yakalar (Ã¶rn., `/about`, `#contact`)
âœ… **BoÅŸ href**: Null/boÅŸ href Ã¶zelliklerini filtreler
âœ… **BÃ¼yÃ¼k Sayfalar**: 1000+ baÄŸlantÄ±lÄ± sayfalarÄ± iÅŸler

---

## ğŸš€ DaÄŸÄ±tÄ±m ve Otomasyon

### ZamanlanmÄ±ÅŸ Ã‡alÄ±ÅŸtÄ±rma (Linux/Mac)

GÃ¼nlÃ¼k baÄŸlantÄ± Ã§Ä±karma iÃ§in crontab'a ekleyin:
```bash
0 2 * * * cd /path/to/linkSaver && /usr/bin/python3 linkSaver.py
```

### Windows GÃ¶rev ZamanlayÄ±cÄ±

Bir batch dosyasÄ± oluÅŸturun (`run_linksaver.bat`):
```batch
@echo off
cd C:\Projects\linkSaver
python linkSaver.py
```

Otomatik Ã§alÄ±ÅŸtÄ±rma iÃ§in GÃ¶rev ZamanlayÄ±cÄ± ile planlayÄ±n.

### Docker DaÄŸÄ±tÄ±mÄ±

```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY linkSaver.py .
RUN pip install requests beautifulsoup4
CMD ["python", "linkSaver.py"]
```

---

## ğŸ—ºï¸ Yol HaritasÄ± ve Gelecek GeliÅŸtirmeler

### Planlanan Ã–zellikler

- [ ] **Komut SatÄ±rÄ± ArayÃ¼zÃ¼**: URL ve Ã§Ä±ktÄ± dosyasÄ± argÃ¼manlarÄ± iÃ§in argparse ekleme
- [ ] **Hata YÃ¶netimi**: AÄŸ hatalarÄ± ve geÃ§ersiz URL'ler iÃ§in try-except bloklarÄ± uygulama
- [ ] **BaÄŸlantÄ± DoÄŸrulama**: Her Ã§Ä±karÄ±lan baÄŸlantÄ± iÃ§in HTTP durum kodu kontrolÃ¼ ekleme
- [ ] **Format SeÃ§enekleri**: JSON, CSV ve XML Ã§Ä±ktÄ± formatlarÄ±nÄ± destekleme
- [ ] **Ã–zyinelemeli Tarama**: BaÄŸlantÄ±larÄ± takip etme ve birden fazla sayfadan Ã§Ä±karma seÃ§eneÄŸi
- [ ] **Filtreleme SeÃ§enekleri**: Domain, protokol (http/https) veya URL desenlerine gÃ¶re filtreleme
- [ ] **Ä°lerleme GÃ¶stergesi**: BÃ¼yÃ¼k siteler iÃ§in tqdm kullanarak ilerleme Ã§ubuÄŸu ekleme
- [ ] **Asenkron Ä°stekler**: Daha hÄ±zlÄ± toplu kazÄ±ma iÃ§in asenkron HTTP istekleri uygulama
- [ ] **GUI ArayÃ¼zÃ¼**: Teknik olmayan kullanÄ±cÄ±lar iÃ§in Tkinter tabanlÄ± masaÃ¼stÃ¼ uygulamasÄ±
- [ ] **BaÄŸlantÄ± Kategorilendirme**: BaÄŸlantÄ±larÄ± dahili, harici, resim veya belge olarak sÄ±nÄ±flandÄ±rma
- [ ] **robots.txt Uyumu**: Web sitesi tarama kurallarÄ±na otomatik uyum
- [ ] **HÄ±z SÄ±nÄ±rlama**: Hedef sunucularÄ± bunaltmamak iÃ§in gecikmeler uygulama

### Bilinen SÄ±nÄ±rlamalar

- AÄŸ hatalarÄ± veya geÃ§ersiz URL'ler iÃ§in hata yÃ¶netimi yok
- JavaScript ile oluÅŸturulan iÃ§eriÄŸi iÅŸleyemez (SPA uygulamalarÄ±)
- Kimlik doÄŸrulama gerektiren sayfalar iÃ§in destek yok
- TÃ¼m baÄŸlantÄ±larÄ± filtreleme yapmadan Ã§Ä±karÄ±r, tekrarlarÄ± kaldÄ±rmaz
- GÃ¶receli URL'ler mutlak URL'lere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmez

*Bu sÄ±nÄ±rlamalar, geliÅŸtirme fÄ±rsatlarÄ±nÄ± temsil eder ve Ã¼retim kalitesindeki gereksinimlerin farkÄ±ndalÄ±ÄŸÄ±nÄ± gÃ¶sterir*

---

## ğŸ¤ KatkÄ±da Bulunma

KatkÄ±lar, sorunlar ve Ã¶zellik istekleri kabul edilir!

### NasÄ±l KatkÄ±da Bulunulur

1. Depoyu fork edin
2. Bir Ã¶zellik dalÄ± oluÅŸturun (`git checkout -b feature/MuhteÅŸemÃ–zellik`)
3. DeÄŸiÅŸikliklerinizi commit edin (`git commit -m 'MuhteÅŸem bir Ã¶zellik ekle'`)
4. DalÄ± push edin (`git push origin feature/MuhteÅŸemÃ–zellik`)
5. Bir Pull Request aÃ§Ä±n

### GeliÅŸtirme Ä°lkeleri

- PEP 8 stil kÄ±lavuzlarÄ±nÄ± takip edin
- KarmaÅŸÄ±k mantÄ±k iÃ§in yorumlar ekleyin
- PR gÃ¶ndermeden Ã¶nce deÄŸiÅŸiklikleri test edin
- Yeni Ã¶zellikler iÃ§in README'yi gÃ¼ncelleyin

---

## ğŸ“„ Lisans

Bu proje MIT LisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r - gerektiÄŸinde kullanmakta, deÄŸiÅŸtirmekte ve daÄŸÄ±tmakta Ã¶zgÃ¼rsÃ¼nÃ¼z.

---

## ğŸ‘¨â€ğŸ’» Yazar

**Enes Uca**

- ğŸŒ Portfolio: [ensuca.github.io](https://ensuca.github.io/ensuca.githubio)
- ğŸ’¼ LinkedIn: [linkedin.com/in/enes-uca-41039327b](https://www.linkedin.com/in/enes-uca-41039327b)
- ğŸ™ GitHub: [@ensuca](https://github.com/ensuca)

---

## ğŸ™ TeÅŸekkÃ¼rler

- **BeautifulSoup4**: Bu mÃ¼kemmel ayrÄ±ÅŸtÄ±rma kÃ¼tÃ¼phanesini oluÅŸturduÄŸu iÃ§in Leonard Richardson'a
- **Requests**: Zarif HTTP kÃ¼tÃ¼phanesi iÃ§in Kenneth Reitz'e
- **Python TopluluÄŸu**: KapsamlÄ± dokÃ¼mantasyon ve destek iÃ§in

---

## ğŸ“ Destek

Sorunlarla karÅŸÄ±laÅŸÄ±rsanÄ±z veya sorularÄ±nÄ±z varsa:

1. Mevcut [GitHub Issues](https://github.com/ensuca/linkSaver/issues) sayfasÄ±nÄ± kontrol edin
2. DetaylÄ± aÃ§Ä±klama ile yeni bir issue oluÅŸturun
3. Profesyonel sorular iÃ§in LinkedIn Ã¼zerinden iletiÅŸime geÃ§in

---

<div align="center">

**â­ FaydalÄ± bulduysanÄ±z bu depoyu yÄ±ldÄ±zlayÄ±n!**

*Python ve temiz kod tutkusuyla inÅŸa edilmiÅŸtir*

</div>
